cs   analysis of algorithms  professor eric aaron  lecture  m w pm  lecture meeting location davis   business  how many of you  won't be here for  monday's class   sa due monday nov    sa out today also due monday nov     ps-lookahead out today   please get started on it   ps due nov     project  out due pm monday nov      yes that's a lot due nov   would you prefer something  due earlier   project  grading update   please meet with me if you'd like prompt feedback on any part of   project   cs --     how to make an algorithm not just a   little faster but a lot faster   making algorithms a little faster changing the leading   constant or handling special cases is good keep doing it   making algorithms a lot faster can be even better   we'll discuss one important technique dynamic programming   primarily used when there's a recursive definition at the center of an   algorithm and it's slowing things down   dynamic programming speeds things up by getting rid of redundant   work without changing the main ideas of the algorithm   can turn algos from exponential time into polynomial time  illustrative example  fibonacci numbers   consider divide-and-conquer and dynamic programming   approaches to calculating the fibonacci numbers   fibonacci definition   f   f     fn  fn-  fn- for n    note there are two base cases   having more than one base case  is not uncommon with recursive  definitions    straightforward top-down recursive approach  rfibonacci n     if n   or n   then return n    else return rfibonaccin-  rfibonaccin-  cs --     cs --   even more illustrative example   fibonacci numbers   straightforward top-down recursive approach  rfibonacci n      if n   or n   then return n     else return rfibonaccin-  rfibonaccin-   where are its inefficiencies  this is an  exponential  algorithm see  clrs pg   for  some of the math  behind that and  see me for the rest   one possible dynamic programming approach a memoized recursive    approach uses an auxiliary table to store results  mfibonaccin       how would we write a memoized  recursive fibonacci function  even more illustrative example   fibonacci numbers   memoized version of fibonacci method  mfibonaccin  mfib-auxfn  let f  n be a new array      f     if n            f   for j    n  fj  - or other sentinel  return mfib-auxfn  if fn   return fn        else     fn  mfib-auxfn-   mfib-auxfn-     return fn   example of a time-memory or time-space trade-off   what's the time complexity   what would a bottom-up fibonacci approach look like    even more illustrative example   fibonacci numbers   straightforward top-down recursive approach  rfibonacci n      if n   or n   then return n     else return rfibonacci n-  rfibonacci n-   even more efficient bottom-up approach  fibonaccin    f   f      for  i    n      fi  fi-  fi-    return fn   could we do even better in what ways   in what ways is this more  efficient   what is the time complexity  of this method the space  complexity  even more illustrative example   fibonacci numbers   straightforward top-down recursive approach  rfibonacci n      if n   or n   then return n     else return rfibonacci n-  rfibonacci n-   even more efficient bottom-up approach  fibonaccin    f   f      for  i    n         temp  f  f  f  f f  temp    return temp   in what ways is this more  efficient   what is the time complexity  of this method the space  complexity  cs --     dynamic programming   the inefficient fibonacci algorithms had a lot of recursive calls    with a lot of redundant work the same calculations were done many   times e g  calculate fib as part of calculating fib   dynamic programming techniques can be more efficient for   problems that have overlapping sub-problems  divide-and-conquer methods recursively solve sub-problems then   combine sub-solutions into a solution   when there are overlapping repeating sub-problems some of this   work can be redundant   dynamic programming methods can solve each sub-problem once store   results in a table o lookup and use the table for later solutions  for the most part   the efficient fibonacci methods used a characteristic   technique of dynamic programming  results stored in a table or similar used to improve efficiency   dynamic programming solutions can be either top-down or   bottom-up  but most of the time in practice when people talk about a dynamic   programming solution they mean a bottom-up solution    in general when looking for a dynamic programming  solution  try recursive top-down approach with overlapping sub-problems   consider a memoized version   then try bottom-up iterative approach based on sub-problems   then try to improve on space complexity of bottom-up method  cs --     for the most  part    dynamic programming is often applied to optimization problems to find a   solution with an optimal minimal or maximal value  often for optimization problems it is or seems necessary to consider all subsets of a set    so if we're looking at a set of size n what's the time complexity of such an algorithm   characteristic structure for dynamic programming algorithms   overlapping subproblems as previously seen   optimal substructure an optimal solution is built from the optimal solutions of   subproblems  this makes total sense if you think about it for a while if there isn't redundant work in  the algo or if optimal solutions aren't based on optimal solutions to subproblems then  why would we store solutions to subproblems  for the most  part    dynamic programming is often applied to optimization problems to find a   solution with an optimal minimal or maximal value  often for optimization problems it is or seems necessary to consider all subsets of a set    so if we're looking at a set of size n what's the time complexity of such an algorithm   characteristic structure for dynamic programming algorithms   overlapping subproblems as previously seen   optimal substructure an optimal solution is built from the optimal solutions of   subproblems  this makes total sense if you think about it for a while if there isn't redundant work in  the algo or if optimal solutions aren't based on optimal solutions to subproblems then  why would we store solutions to subproblems   steps in developing a dynamic programming algorithm     characterize the structure of an optimal solution in words    recursively define the value of an optimal solution    compute the value of an optimal solution from the bottom up    construct an optimal solution from computed information  these are on  clrs pg     we'll focus on  steps  and   leading to step   cs --     cs --     sequences and subsequences   another category of problems involves sequences and   subsequences   definition    given sequence x  x x  xm another sequence   z  z z   zk is a subsequence of x if    there exists a strictly increasing sequence  iiik of   indices of x s t  for all j in   k xij  zj   i e  elements of x preserving order   example  definition   if x  abacab and y  abba    then aa ab ba bb aba abb and   others are all subsequences of  both x and y i e  they are  common subsequences  longest common subsequence    if x  abcbdab and y  bdcaba then  bca is a common subsequence but not a longest  common subsequence lcs   bcba and bcab are both longest common   subsequences of x y   there is no common subsequence of length    the longest common subsequence problem    given x   x x     xm  and y  y y    yn find a longest   common subsequence lcs of x and y    what's the brute force way to solve this what's its time   complexity and what can we do about that  dynamic programming to the rescue   the lcs problem seems like a candidate for a dynamic programming   solution   does it look like a recursive definition of lcs would be helpful hint   yes    is there optimal substructure can the lcs of two sequences x and y be   built from the lcs of subsequences divide and conquer   are there overlapping sub-problems are there redundant calculations in a   recursive solution   as part of a recursive  divide-and-conquer definition we'll refer to the   i'th prefix of a sequence   definition  notation given a sequence x  x x  xm the i'th  prefix for i in   m is xi  x x  xi  so in our notation see prev  slide x   x x     xm   xm and   y  y y    yn  yn  cs --     lcs a recursive solution   let x   x x    xm  and y   y y    yn  be sequences   and let z   z z    zk  be any lcs of x and y    how does z relate to lcss of x and y or to lcss of   prefixes of x and y   how does this lead to a recursive solution for the length of an   lcs what subproblems need to be solved  lcs a recursive solution   let x   x x    xm  and y   y y    yn  be sequences   and let z   z z    zk  be any lcs of x and y    how does z relate to lcss of x and y or to lcss of   prefixes of x and y  case   if xm  yn then zk  xm  yn and zk- is an lcs of xm- and yn-  case   if xm  yn then if zk  xm then z is an lcs of xm- and y  case   if xm  yn then if zk  yn then z is an lcs of x and yn- do you see overlapping subproblems from this formulation   how does this lead to a recursive solution for the length of an   lcs what subproblems need to be solved  cs --     lcs a recursive solution   next step recursively find the length of the longest common   subsequence of x y  how can we do that based on our three cases   case   if xm  yn then zk  xm  yn and zk- is an lcs of xm- and yn-  case   if xm  yn then if zk  xm then z is an lcs of xm- and y  case   if xm  yn then if zk  yn then z is an lcs of x and yn-  note we need to track lengths of lcss of various sub-problems   use cij to store the length of lcs of xi yj  goal compute cmn  what's the base case for this recursion  the recursion is over sequences what's  the smallest sequence we might consider  lcs a recursive solution   next step recursively find the length of the longest common   subsequence of x y  how can we do that based on our three cases   case   if xm  yn then zk  xm  yn and zk- is an lcs of xm- and yn-  case   if xm  yn then if zk  xm then z is an lcs of xm- and y  case   if xm  yn then if zk  yn then z is an lcs of x and yn-  note we need to track lengths of lcss of various sub-problems   use cij to store the length of lcs of xi yj  goal compute cmn   recursive formulation   base case cj   and ci   for all i j   recursive  step to compute cij where ij     if xi  yj cij  ci-j-    if xi  yj cij  maxcij- ci-j  this reflects  uses the three  subproblems noted above  cs --     cs --   lcs a recursive solution   recursively find the length of the lcs of x y   base case cj   and ci   for all i j   recursive  step to compute cij where ij     if xi  yj cij  ci-j-    if xi  yj cij  maxcij- ci-j  i e  one less elt  of both x and y  i e  one less elt  of either x or y   straightforward recursive code   initialize ci  cj   for i in   m j in   n   initialize cij  nil for i in   m j in   n  initialize c including  the base cases  lcsi j  i j are indices  this is for a particular i j    if ci j  nil  could use other sentinel value         then if xi  yj                     then ci j  lcsi- j-    one less of both xy                     else ci j  maxlcs i j- lcs i- j    return ci j  for the most even more   recall steps to developing a dyn  prog  algorithm     characterize the structure of an optimal solution in words    recursively define the value of an optimal solution    compute the value of an optimal solution from the bottom up    construct an optimal solution from computed information   we've done the first two  now   how to compute the value of an optimal solution i e  the length of a  longest common subsequence with bottom-up design instead of top- down recursion perhaps using a table to avoid redundant work   stay with the same basic ideas just expressed in a different design   what additional information would support constructing a solution an   actual longest common subsequence from the computation of the  optimal value    cs --   bottom-up computation of optimal lcs   value   need m-by-n matrix c to store lengths   actually m-by-n to  include the  case too   to compute cij need values of ci-j- when xi  yj and   ci- j and ci j- when xi  yj   recall our recursive definition   base case cj   and ci   for all i j recursive  step to compute cij for ij    if xi  yj cij  ci-j-    if xi    yj cij  maxcij- ci-j   what is the time complexity of this algorithm   lcsx y  input sequences x y    m  lengthx    n  lengthy    for i   to m do  ci       in first col of each row    for j   to n  do  c j     in first row of each col    for i   to m do        for j   to n do     process row by row             if xi  yj then ci j  ci- j-               else ci j  max ci j- ci- j    return cm n   what is the optimal  length the length  of an lcs of full  sequences  x and y  bottom-up computation of an optimal lcs   to find an lcs also store which symbols indices of symbols   are actually part of the lcs as it's being built  i e   which table elements have optimal sub-problem values   if xi  yj  answer came from the upper left diagonal of current element i e  one less elt  of both x and y   if xi  yj the answer came from above or to the left whichever is larger   if equal we can choose above by convention  i e  either x or y  lcsx y    m  lengthx    n  lengthy    for i   to m do  ci       for j   to n  do  c j      for i   to m do        for j   to n do  bi j  upleft  if xi  yj then ci j  ci- j-    if ci -  j   ci j -  then            else                   bi j  up one less elt  of x      ci j  ci -  j  else ci  j  ci j -   bi j  left one less elt  of y     lcs example  lcs of  x  abcbdab and  y  bdcaba  the algorithm finds  an lcs bcba  are there others  and finally  finding a solution from the values   that bottom-up method gives us the information from which   we can get an optimal value and the associated indices   to actually find  print the longest common subsequence  start at the bottom-left of the table and follow the arrows  print- lcs bxij     if i   or j   then return    if bij  upleft         then print-lcsbxi-j-                  print xi    else if bij  up         then print-lcsbxi-j    else print-lcsbxij-  initial call has   i  m i e  lengthx  j  n lengthy     b is the arrow table from  the previous slide  cs --     