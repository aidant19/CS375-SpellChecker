cs   analysis of algorithms  professor eric aaron  lecture  m w pm  lecture meeting location davis   business   smaller assignment  returned already   let me know if there are problems accessing it  please read the emailed  classwide comments   smaller assignment  due already   problem set  out due sept     project  due sept     please direct project-specific questions to me rather than to tas   questions about general concepts that show up on the project e g    theta notation though rather than specifics can go to tas   everyone was on a team as of yesterday   let me know if there are problems  concerns with team assignments  cs --     business pt     class will be cancelled monday sept     will be an optional make-up class later in the semester   let's go over sa exercise  f   if axyz and bxy what is axb   axb is by definition a set of ordered pairs please be sure to use the   correct notation and concepts notation and semantics matter to cs  just ask your compiler  asymptotic analysis    big-o notation   with insertion sort if we gloss over minor details we can see   the number of operations worst case is on the order of n   i e  it is cn  lower order terms   for some constant c    where n is the size of the input   definition an algorithm runs in time ofn read order of   fn means  there exist c   n   s t    for all n  n the running time of the algorithm is less than cfn  basically that means that for every input big enough the running   so we'd say insertion sort is on  time is less than a constant times fn  cs --     asymptotic analysis    big-o notation   definition an algorithm runs in time ofn read order of   fn means  there exist c   n   s t    for all n  n the running time of the algorithm is less than cfn  basically that means that for every input big enough the running   defn  repeated from prev  slide  time is less than a constant times fn informal intuition big-o is about     upper bounds   if a runtime tn is ofn then for   big enough n tn is upper bounded by  cfn for some leading constant c  note this figure from your textbook uses fn  for runtime and gn for the bounding function  but it's the same idea fn is ogn upper  bounded by cgn for all n  n  breaking down the phrase   big-o asymptotic complexity   major takeaways about big-o asymptotic complexity      in fact there's one major takeaway for each of the three words in the  phrase big-o asymptotic complexity based on their meaning   it's best to work from the end of that phrase to the beginning   complexity it's about describing the   resource usage of an algorithm   asymptotic it describes complexity based   on behavior on large input sizes n  small inputs aren't really the point   big-o it's an upper bound on complexity  on large inputs  big-o in this picture for large enough n that is n  n fn is upper bounded by a  leading constant c times gn  cs --     asymptotic analysis    big-o notation   definition an algorithm runs in time ofn read order of   fn means  there exist c   n   s t    for all n  n the running time of the algorithm is less than cfn  basically that means that for every input big enough the running   defn  repeated from prev  slide  time is less than a constant times fn  recall big-o is about upper bounds   this runtime measure captures some   essential characteristic of an algorithm  on algorithms differ from on   from on log n etc    can talk about asymptotic complexity classes  we say insertion sort is in complexity class on  conventional wisdom about big-o classes    if two algorithms are in different big-o classes then there seems to be  something substantially different about their speeds  even though for some small values of n an on algorithm could be   faster than an on algorithm   it is nonetheless true that n grows faster than n  thus an on algorithm is in a relevant sense inherently slower than an   on algorithm  important vocab see clrs pg   these functions of n have very different  orders of growth i e  how fast they grow as n gets larger   for an on algorithm called linear   doubling the input size does what to the running time  increasing input size by factor of  does what to running time   for an on algorithm quadratic   doubling the input size does what to the running time  increasing input size by factor of  does what to running time   for an on algorithm exponential   doubling the input size does what to the running time  cs --     cs --   common complexity measures and   how they relate to input sizes   algorithms are sometimes   described by their time  complexity  there are  logarithmic algorithms  quadratic algorithms  exponential algorithms  factorial algorithms  etc    to see which kind is  fastest see how these  functions grow with  increases in the input size  log n n  n          n    n                    e  e        e  e    using the big-o definition   definition ogn  fn  exists c n   s t  forall n  n    fn  cgn    is each of the below statements true explain your answers                          n    on n - n  on n  on n  on  n  on n lg n  olg n n  on n  on  using the big-o definition   definition ogn  fn  exists c n   s t  forall n  n    fn  cgn    is each of the below statements true explain your answers                          n    on n - n  on n  on n  on  n  on n lg n  olg n n  on n  on  pro tip on how to explain these in general when explaining why an  existential exists statement is  true explicitly give some witness values that make it true as part of  the explanation   here if a statement is true can   you give specific values for c n that make it true  cs --     cs --   big oh there's more notation   theta notation asymptotically tight bound  definition gn  fn  exists c c n     s t  forall n  n   cgn  fn  cgn  big oh there's more notation   theta notation asymptotically tight bound  definition gn  fn  exists c c n     s t  forall n  n   cgn  fn  cgn  reminder--defn of big-o ogn  fn  exists c n     s t  forall n  n   fn  cgn    big oh there's more notation   theta notation asymptotically tight bound  definition gn  fn  exists c c n     s t  forall n  n   cgn  fn  cgn  reminder--defn of big-o ogn  fn  exists c n     s t  forall n  n   fn  cgn   big-omega notation asymptotic lower bound   definition gn  fn  exists c n     s t  forall n  n   cgn  fn  big oh there's more notation   theta notation asymptotically tight bound   definition gn  fn  exists c c n   s t  forall n  n     cgn  fn  cgn   big-omega notation asymptotic lower bound    definition gn  fn  exists c n   s t  forall n  n    cgn  fn   what is the relationship among big-o big-omega and theta   classes  cs --     a big-symbols theorem   definition gn  fn  exists c c n   s t  forall n  n    cgn  fn  cgn   definition gn  fn  exists c n   s t  forall n  n    cgn  fn   theorem for any two functions fn and gn fn  gn iff fn    ogn and fn  gn   using the   definitions   definition gn  fn  exists c c n   s t  forall n  n    cgn  fn  cgn   definition gn  fn  exists c n   s t  forall n  n    cgn  fn    is each of the below statements true                           n    n n    n n - n  n n - n  n n  n  n  n n  n n  n  cs --     cs --   conventions order of growth  to within a constant multiple   two different levels of detail can be useful with asymptotic complexity   formal definitions and detailed explanations   informal high-level understanding and explanations   when informally talking about asymptotic complexity we often talk about   the order of growth of runtime functions to within a leading constant  multiple   we don't say exactly what the leading constant c or n threshold is  order of growth of the highest order  dominant term is most important  in cs unless specified otherwise feel free  to use the informal high-level approach    cs --   log it  questions about exponents   when solving equations we may want to know the value of   an exponent  e g  in equation x we might want to ask what value of x makes   that true   how could we even phrase that question   the logarithm function lets us ask the question   so for x   we'd say x  log  read as log base  of   examples log    log    log      logarithms are exponents so rules of exponentiation apply   e g  logb mn  logb m  logb n  if bx  m and by  n  then bxby  bxy  mn    